# -*- coding: utf-8 -*-
"""ensemble_current.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r0SXWJKSbgjOesyJXCs7A7ATN8b8da-g
"""


from sklearn.preprocessing import MinMaxScaler
import os
from sklearn.metrics import confusion_matrix
import keras
import sklearn
import tempfile
from sklearn.model_selection import StratifiedKFold
import tensorflow as tf
from sklearn.preprocessing import *
import pandas as pd
import matplotlib as mpl
import re
import matplotlib.pyplot as plt
from keras import backend as kf
import numpy as np
import seaborn as sns
import string
from sklearn.utils import class_weight
from sklearn.model_selection import train_test_split
import os
from transformers import BertTokenizer, TFBertModel, BertConfig
from transformers import DebertaTokenizer, TFDebertaForMaskedLM
from transformers import DistilBertTokenizer, TFDistilBertModel, DistilBertConfig
from keras import backend as K
import inflect
from keras import regularizers
import sklearn as sk
from keras.layers import Activation, Dense
from tensorflow import keras
from keras.utils.vis_utils import plot_model
from focal_loss import BinaryFocalLoss
from tensorflow import keras

# colors for confusion matrix and size of figures
mpl.rcParams['figure.figsize'] = (12, 10)
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']

# load tokenizer
#tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# load dataset
df = pd.read_csv('jti_dataset_refined.csv')

# define strategy for gpu distributed workload sclaes with number of gpus available
# strategy = tf.distribute.MirroredStrategy()
# strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
# gpus = tf.config.list_logical_devices('GPU')
tf.config.set_soft_device_placement(True)
strategy = strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))

# define maxlength of bert vector
MAXLEN = 200


# data cleaning
def text_clean(x):
    x = x.lower()  # lowercase everything
    x = x.encode('ascii', 'ignore').decode()  # remove unicode characters
    x = re.sub(r'https*\S+', ' ', x)  # remove links
    x = re.sub(r'http*\S+', ' ', x) # remove links
    return x


# apply to dataset and test to check on example
print(df['Cleaned_Idea_Description'][2314])
df['Cleaned_Idea_Description'] = df['Cleaned_Idea_Description'].apply(text_clean)
print(df['Cleaned_Idea_Description'][2314])

# use inflect (to later convert number to written numbers e.g. 4 -> four)
p = inflect.engine()

# convert number into words
def convert_number(text):
    # split string into list of words
    temp_str = text.split()
    # initialise empty list
    new_string = []

    for word in temp_str:
        # if word is a digit, convert the digit
        # to numbers and append into the new_string list
        if word.isdigit():
            temp = p.number_to_words(word)
            new_string.append(temp)

        # append the word as it is
        else:
            new_string.append(word)

    # join the words of new_string to form a string
    temp_str = ' '.join(new_string)
    return temp_str

# apply to dataset and test to check on example
df['Cleaned_Idea_Description'] = df['Cleaned_Idea_Description'].apply(convert_number)
print(df['Cleaned_Idea_Description'][2314])

# remove punctuation
def remove_punctuation(text):
    translator = str.maketrans('', '', string.punctuation)
    return text.translate(translator)

# remove whitespace from text
def remove_whitespace(text):
    return " ".join(text.split())

# apply to dataset and test to check on example
df['Cleaned_Idea_Description'] = df['Cleaned_Idea_Description'].apply(remove_whitespace)
print(df['Cleaned_Idea_Description'][2314])

# ratio of good to bad ideas
neg, pos = np.bincount(df['top_ideas'])
total = neg + pos
print('Examples:\n    Total: {}\n    Positive: {} ({:.2f}% of total)\n'.format(
    total, pos, 100 * pos / total))

weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = ((1 / pos) * (total / 2.0))

# calculate class weights
class_weight = {0: weight_for_0, 1: weight_for_1}
# calculate class weights
print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))

# calculate initial weights
initial_bias = np.log([pos / neg])
# colors for confusion matrix
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']

# hyperparameters for training (batchsize should be 64, e.g. with 1 gpu batch_size_per_replica = 32)
MAXLEN = MAXLEN
BATCH_SIZE_PER_REPLICA = 32
BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync
EPOCHS = 50
DROPOUT = 0.3
LEARNING_RATE = 3e-5
REGULARIZATION = 0.01
DATA_LENGTH = len(df)


class CustomCallback(keras.callbacks.Callback):
    def on_train_begin(self, logs=None):
        keys = list(logs.keys())
        print("Starting training; got log keys: {}".format(keys))

    def on_train_end(self, logs=None):
        keys = list(logs.keys())
        print("Stop training; got log keys: {}".format(keys))

    def on_epoch_begin(self, epoch, logs=None):
        keys = list(logs.keys())
        print("Start epoch {} of training; got log keys: {}".format(epoch, keys))

    def on_epoch_end(self, epoch, logs=None):
        keys = list(logs.keys())
        print("End epoch {} of training; got log keys: {}".format(epoch, keys))

    def on_test_begin(self, logs=None):
        keys = list(logs.keys())
        print("Start testing; got log keys: {}".format(keys))

    def on_test_end(self, logs=None):
        keys = list(logs.keys())
        print("Stop testing; got log keys: {}".format(keys))

    def on_predict_begin(self, logs=None):
        keys = list(logs.keys())
        print("Start predicting; got log keys: {}".format(keys))

    def on_predict_end(self, logs=None):
        keys = list(logs.keys())
        print("Stop predicting; got log keys: {}".format(keys))

    def on_train_batch_begin(self, batch, logs=None):
        keys = list(logs.keys())
        print("...Training: start of batch {}; got log keys: {}".format(batch, keys))

    def on_train_batch_end(self, batch, logs=None):
        keys = list(logs.keys())
        print("...Training: end of batch {}; got log keys: {}".format(batch, keys))

    def on_test_batch_begin(self, batch, logs=None):
        keys = list(logs.keys())
        print("...Evaluating: start of batch {}; got log keys: {}".format(batch, keys))

    def on_test_batch_end(self, batch, logs=None):
        keys = list(logs.keys())
        print("...Evaluating: end of batch {}; got log keys: {}".format(batch, keys))

    def on_predict_batch_begin(self, batch, logs=None):
        keys = list(logs.keys())
        print("...Predicting: start of batch {}; got log keys: {}".format(batch, keys))

    def on_predict_batch_end(self, batch, logs=None):
        keys = list(logs.keys())
        print("...Predicting: end of batch {}; got log keys: {}".format(batch, keys))


        
#early stopping for bert finetuning
# see Zhou et al. 2020
class EarlyStoppingAtMinLoss(keras.callbacks.Callback):
    
    def __init__(self, patience=0):
        #when initiated define the arrays for different metrics
        super(EarlyStoppingAtMinLoss, self).__init__()
        self.patience = patience
        self.epoch = 0
        self.sp = []
        self.recall = []
        self.accuracy = []
        self.sp_val = []
        self.recall_val = []
        self.accuracy_val = []
        # best_weights to store the weights at which the minimum loss occurs.
        self.best_weights = None

    def on_train_begin(self, logs=None):
        # The number of epoch it has waited when loss is no longer minimum.
        self.wait = 0
        # The epoch the training stops at.
        self.stopped_epoch = 0
        # Initialize the best as infinity.
        self.best = 0
    
    def on_epoch_end(self, epoch, logs=None):
        #after every epoch append new metrics to arrays
        self.sp.append(logs.get("special_metric"))
        self.recall.append(logs.get("recall"))
        self.accuracy.append(logs.get("accuracy"))

        self.epoch = epoch

        self.sp_val.append(logs.get("val_special_metric"))
        self.recall_val.append(logs.get("val_recall"))
        self.accuracy_val.append(logs.get("val_accuracy"))

        try:
            #logic for early sopping
            if self.sp_val[epoch] < self.sp_val[epoch - 1]:
                if self.recall[epoch] < 0.8 or self.accuracy[epoch] < 0.8:
                    pass
                elif self.recall_val[epoch] > self.recall_val[epoch - 1]:
                    pass
                else:
                    if self.sp_val[epoch - 1] > self.sp_val[epoch - 2] and self.sp_val[epoch] > self.sp_val[epoch - 2]:
                        pass
                    else:
                        self.model.stop_training = True
                        self.stopped_epoch = epoch
                        print("Restoring model weights from the end of the best epoch.")
                        self.model.set_weights(self.best_weights)
            else:
                if self.sp_val[epoch] > self.best:
                    self.best_weights = self.model.get_weights()
        except TypeError:
            print("first_round")

    def on_train_end(self, logs=None):
        if self.stopped_epoch > 0:
            print("Epoch %05d: early stopping\n" % (self.stopped_epoch + 1))


class EarlyStoppingAtMinLoss_mlp(keras.callbacks.Callback):

    def __init__(self, patience=0):
        #when initiated define the arrays for different metrics
        super(EarlyStoppingAtMinLoss_mlp, self).__init__()
        self.patience = patience
        self.epoch = 0
        self.sp = []
        self.recall = []
        self.accuracy = []
        self.sp_val = []
        self.recall_val = []
        self.accuracy_val = []
        # best_weights to store the weights at which the minimum loss occurs.
        self.best_weights = None

    def on_train_begin(self, logs=None):
        # The number of epoch it has waited when loss is no longer minimum.
        self.wait = 0
        # The epoch the training stops at.
        self.stopped_epoch = 0
        # Initialize the best as infinity.
        self.best = 0

    def on_epoch_end(self, epoch, logs=None):
        #after every epoch append new metrics to arrays
        self.sp.append(logs.get("special_metric"))
        self.recall.append(logs.get("recall"))
        self.accuracy.append(logs.get("accuracy"))

        self.epoch = epoch

        self.sp_val.append(logs.get("val_special_metric"))
        self.recall_val.append(logs.get("val_recall"))
        self.accuracy_val.append(logs.get("val_accuracy"))

        try:
            #logic for early sopping
            if self.sp_val[epoch] < self.sp_val[epoch - 1]:
                if self.recall[epoch] < 0.75 or self.accuracy[epoch] < 0.75:
                    pass
                elif self.recall_val[epoch] > self.recall_val[epoch - 1]:
                    pass
                else:
                    if self.sp_val[epoch - 1] > self.sp_val[epoch - 2] and self.sp_val[epoch] > self.sp_val[epoch - 2]:
                        pass
                    else:
                        self.model.stop_training = True
                        self.stopped_epoch = epoch
                        print("Restoring model weights from the end of the best epoch.")
                        self.model.set_weights(self.best_weights)
            else:
                if self.sp_val[epoch] > self.best:
                    self.best_weights = self.model.get_weights()
        except TypeError:
            print("first_round")

    def on_train_end(self, logs=None):
        if self.stopped_epoch > 0:
            print("Epoch %05d: early stopping\n" % (self.stopped_epoch + 1))


#custom metric as a weighted average from recall and accuracy
class Special_Metric(tf.keras.metrics.Metric):

    def __init__(self, name='special_metric', **kwargs):
        super(Special_Metric, self).__init__(name=name, **kwargs)
        self.true_positives = self.add_weight(name='tp', initializer='zeros')
        self.false_positives = self.add_weight(name='fp', initializer='zeros')
        self.true_negatives = self.add_weight(name='tn', initializer='zeros')
        self.false_negatives = self.add_weight(name='fn', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.float32)
        tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))
        fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))
        fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))

        self.true_positives.assign_add(tp)
        self.false_positives.assign_add(fp)
        self.true_negatives.assign_add(tn)
        self.false_negatives.assign_add(fn)

    def result(self):
        recall = self.true_positives / (self.true_positives + self.false_negatives)
        accuracy = (self.true_positives + self.true_negatives) / (
                self.true_positives + self.false_negatives + self.true_negatives + self.false_positives)
        special = (1.2 * recall + accuracy) * 0.45
        return special

    def reset_state(self):
        self.true_positives.assign(0)
        self.false_positives.assign(0)
        self.true_negatives.assign(0)
        self.false_negatives.assign(0)


#custom F1 impelementation, as keras has no F1 metric of its own
class F1(tf.keras.metrics.Metric):

    def __init__(self, name='F1', **kwargs):
        super(F1, self).__init__(name=name, **kwargs)
        self.true_positives = self.add_weight(name='tp', initializer='zeros')
        self.false_positives = self.add_weight(name='fp', initializer='zeros')
        self.true_negatives = self.add_weight(name='tn', initializer='zeros')
        self.false_negatives = self.add_weight(name='fn', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.float32)
        tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))
        fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))
        fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))

        self.true_positives.assign_add(tp)
        self.false_positives.assign_add(fp)
        self.true_negatives.assign_add(tn)
        self.false_negatives.assign_add(fn)

    def result(self):
        recall = self.true_positives / (self.true_positives + self.false_negatives)
        precision = self.true_positives / (self.true_positives + self.false_positives)
        f1 = 2 * precision * recall / (precision + recall)
        return f1

    def reset_state(self):
        self.true_positives.assign(0)
        self.false_positives.assign(0)
        self.true_negatives.assign(0)
        self.false_negatives.assign(0)

#define metrics to be monitored during training
with strategy.scope():
    METRICS = [
        keras.metrics.TruePositives(name='tp'),
        keras.metrics.FalsePositives(name='fp'),
        keras.metrics.TrueNegatives(name='tn'),
        keras.metrics.FalseNegatives(name='fn'),
        keras.metrics.Recall(name='recall'),
        keras.metrics.BinaryAccuracy(name='accuracy'),
        tf.keras.metrics.Precision(name='precision'),
        keras.metrics.AUC(name='auc'),
        keras.metrics.AUC(name='prc', curve='PR'),
        Special_Metric()
    ]


BERT_DROPOUT = 0.3
BERT_ATT_DROPOUT = 0.3
# Configure BERT's initialization (increase dropout compared to standard model)
config_bert= BertConfig(dropout=BERT_DROPOUT,
                               attention_dropout=BERT_ATT_DROPOUT,
                               output_hidden_states=True)


# bert for text classification (as in Devlin et al. 2018)
#alternatively for a lighter but slightly worse performing classification use distilbert (see Sanh et al. 2020)
def build_bert(transformer,seed, max_len=MAXLEN):
    #weight initializer is fixed to ensure replicability
    weight_initializer = tf.keras.initializers.GlorotNormal(seed=seed)
    output_bias = tf.keras.initializers.Constant(initial_bias)
    input_word_ids = tf.keras.layers.Input(
        shape=(max_len,), dtype=tf.int32, name="input_word_ids"
    )
    sequence_output = transformer(input_word_ids)[0]
    cls_token = sequence_output[:, 0, :]
    #add one layer of dropout after the classic bert
    net = tf.keras.layers.Dropout(0.2)(cls_token)
    #add one layer of 256 neurons
    dense1 = tf.keras.layers.Dense(256, kernel_initializer='ones', )(net)
    #output as sigmoid for binary classification
    out = tf.keras.layers.Dense(1, activation="sigmoid",
                                kernel_initializer=weight_initializer,
                                #bias initializer is calculated from the negative to positive class balance for faster convergence
                                bias_initializer=output_bias)(dense1)
    model = tf.keras.models.Model(inputs=input_word_ids, outputs=out)
    model.compile(
        tf.keras.optimizers.Adam(lr=LEARNING_RATE),
        # Binary focal loss instead of binary cross entropy (see Lin et al. 2018)
        loss=BinaryFocalLoss(gamma=2),
        metrics=METRICS
    )
    return model


#mlp for metadata classification
def build_mlp(metrics=METRICS):
    output_bias = tf.keras.initializers.Constant(initial_bias)
    input = keras.Input(shape=(6, ))
    layer1= Dense(128, activation='relu', 
                kernel_initializer='random_normal')(input)
    dropout1 = keras.layers.Dropout(0.2)(layer1)
    layer2 = Dense(64,activation='relu',kernel_initializer='random_normal')(dropout1)
    dropout2 = keras.layers.Dropout(0.2)(layer2)
    layer3 = Dense(32,activation='relu',kernel_initializer='random_normal')(dropout2)
    output = Dense(1,activation='sigmoid',kernel_initializer='random_normal',bias_initializer=output_bias)(layer3)
    model = tf.keras.models.Model(inputs=input, outputs=output)
    model.compile(
      optimizer=keras.optimizers.Adam(learning_rate=1e-3),
      loss=keras.losses.BinaryCrossentropy(),
      metrics=metrics)
    return model




#mlp for metadata classification
def build_mlp_1(metrics=METRICS):
    output_bias = tf.keras.initializers.Constant(initial_bias)
    input = keras.Input(shape=(6, ))
    layer2 = Dense(64,activation='relu',kernel_initializer='random_normal')(input)
    output = Dense(1,activation='sigmoid',kernel_initializer='random_normal',bias_initializer=output_bias)(layer2)
    model = tf.keras.models.Model(inputs=input, outputs=output)
    model.compile(
      optimizer=keras.optimizers.Adam(learning_rate=1e-3),
      loss=keras.losses.BinaryCrossentropy(),
      metrics=metrics)
    return model


#mlp for metadata classification
def build_mlp_2(metrics=METRICS):
    output_bias = tf.keras.initializers.Constant(initial_bias)
    input = keras.Input(shape=(6, ))
    layer1= Dense(128, activation='relu', 
                kernel_initializer='random_normal')(input)
    dropout1 = keras.layers.Dropout(0.3)(layer1)
    layer3 = Dense(32,activation='relu',kernel_initializer='random_normal')(dropout1)
    output = Dense(1,activation='sigmoid',kernel_initializer='random_normal',bias_initializer=output_bias)(layer3)
    model = tf.keras.models.Model(inputs=input, outputs=output)
    model.compile(
      optimizer=keras.optimizers.Adam(learning_rate=1e-3),
      loss=keras.losses.BinaryCrossentropy(),
      metrics=metrics)
    return model



# combined model (see Ostendorff et al. 2019) (a.t.m. performs worse than the ensemble model)
def combined_model_test(transformer, max_len=MAXLEN, metrics=METRICS):
    output_bias = tf.keras.initializers.Constant(initial_bias)

    # input for bert model
    in_bert = tf.keras.layers.Input(
        shape=(max_len,), dtype=tf.int32, name="in_bert"
    )
    # input the input to the bert model
    sequence_output = transformer(in_bert)[0]
    # take out the cls token from the bert model
    cls_token = sequence_output[:, 0, :]

    # input for the mlp
    in_mlp = keras.Input(shape=(6,))
    # concat result from bert and first mlp layer
    concatted = tf.keras.layers.concatenate([cls_token, in_mlp])
    # dense layer with concatted input
    layer1 = Dense(774, activation='relu', kernel_initializer='random_normal',
                   kernel_regularizer=regularizers.L2(REGULARIZATION))(concatted)
    # dropout for regularization
    dropout1 = tf.keras.layers.Dropout(0.2)(layer1)
    # second dense layer with concatted input
    layer2 = Dense(774, activation='relu', kernel_initializer='random_normal')(dropout1)
    # dropout for regularization
    dropout2 = tf.keras.layers.Dropout(0.2)(layer2)
    # third dense layer with concatted input
    layer3 = Dense(512, activation='relu', kernel_initializer='random_normal')(dropout2)
    # sigmoid for output
    out_all = Dense(1, activation='sigmoid', kernel_initializer='random_normal', bias_initializer=output_bias)(layer3)

    model = tf.keras.models.Model(inputs=[in_bert, in_mlp], outputs=out_all)
    model.compile(
        tf.keras.optimizers.Adam(lr=LEARNING_RATE),
        loss="binary_crossentropy",
        metrics=metrics
    )
    return model


# transform idea text to vector with maxlength attribute, shorter ideas get padded to max_length
def preprocess_text(data):
    """ take texts and prepare as input features for BERT 
    """
    input_ids = []
    # For every sentence...
    for comment in data:
        encoded_sent = tokenizer.encode_plus(

            text=comment,
            add_special_tokens=True,  # Add `[CLS]` and `[SEP]`
            max_length=MAXLEN,  # Max length to truncate/pad
            pad_to_max_length=True,  # Pad sentence to max length
            return_attention_mask=False,  # attention mask not needed for our task
            return_token_type_ids=False
        )
        # Add the outputs to the lists
        input_ids.append(encoded_sent.get("input_ids"))
    return input_ids


#custom function to combine different models e.g. the 3 bert models for the ensemble model, as keras only has average.
# see Ammar & Rania 2021
class HardMax(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(HardMax, self).__init__(**kwargs)

    def call(self, inputs):
        preds_pos = []
        preds_neg = []
        #check if first pred is 1 or 0
        if inputs[0] >= 0.5:
            preds_pos.append(inputs[0])
        else:
            preds_neg.append(inputs[0])
        #check if second pred is 1 or 0
        if inputs[1] >= 0.5:
            preds_pos.append(inputs[1])
        else:
            preds_neg.append(inputs[1])
        #check if third pred is 1 or 0
        if inputs[2] >= 0.5:
            preds_pos.append(inputs[2])
        else:
            preds_neg.append(inputs[2])
        #check if there are more positive or negative predictions
        if len(preds_pos) > len(preds_neg):
        #return average of either positive or negative predictions depending on step above
            return sum(preds_pos) / len(preds_pos)
        else:
            return sum(preds_neg) / len(preds_neg)

#initialize standard scaler
scaler = StandardScaler()

# call preprocess_text to transform text in data to vectors for bert classification
X_bert = np.array(preprocess_text(df["Cleaned_Idea_Description"]))

# create data for mlp classification
X_mlp = np.array(df[['Likes', 'No of Comments', 'Voters', 'Visitors', 'lenNumber', 'wordNumber']])

# transform and normalise data for better classification
scaler.fit_transform(X_mlp)
X_mlp = scaler.transform(X_mlp)
X_mlp = np.clip(X_mlp, -5, 5)

# create labels dataframe
Y = np.array(df['top_ideas'])

# 5 fold corssvalidation on dataset, the random state is only to ensure replicability, stratified fold is used in order to mitigate the unbalanced dataset
# see Refaeilzadeh et al. 2016
kf = StratifiedKFold(n_splits=5, random_state=11, shuffle=True)
kf.get_n_splits(X_bert, Y)

# arrays in order to save metrics through all 5 runs
histories_bert = []
histories_mlp = []
test_preds_bert_array_0 = []
test_preds_bert_array_1 = []
test_preds_bert_array_2 = []
test_preds_mlp_array_0 = []
test_preds_mlp_array_1 = []
test_preds_mlp_array_2 = []
ensemble_bert_preds_array = []
ensemble_mlp_preds_array = []
ensemble_final_preds_array = []
train_labels = []
test_labels = []

#define regularization to be added to pretrained bert models
regularizer = tf.keras.regularizers.l2(REGULARIZATION)

# Loop over the dataset to create seprate folds
for train_index, test_index in kf.split(X_bert, Y):

    # create datasets for this run from all data
    X_bert_train, X_bert_test = X_bert[train_index], X_bert[test_index]
    X_mlp_train, X_mlp_test = X_mlp[train_index], X_mlp[test_index]
    y_train, y_test = Y[train_index], Y[test_index]

    # print model parameters
    steps_per_epoch = int(np.floor((len(X_bert_train) / BATCH_SIZE)))
    print(
        f"Model Params:\nbatch_size: {BATCH_SIZE}\nEpochs: {EPOCHS}\n"
        f"Step p. Epoch: {steps_per_epoch}\n"
        f"Learning rate: {LEARNING_RATE}"
    )

    # append the labels to the arrays
    train_labels.append(y_train)
    test_labels.append(y_test)

    # strategy.scope in order to use multiple gpus
    with strategy.scope():
        # Create pretrained bert models for this run with the config for higher dropout 
        BERT1 = TFBertModel.from_pretrained("bert-base-uncased", config=config_bert)
        BERT2 = TFBertModel.from_pretrained("bert-base-uncased", config=config_bert)
        BERT = TFBertModel.from_pretrained("bert-base-uncased", config=config_bert)
        # Create the mlps for this run
        model_mlp_0 = build_mlp()
        model_mlp_1 = build_mlp_1()
        model_mlp_2 = build_mlp_2()

        # Make pretrained bert layers untrainable
        for layer in BERT.layers:
            layer.trainable = False
        for layer in BERT1.layers:
            layer.trainable = False
        for layer in BERT2.layers:
            layer.trainable = False

        #add regularization to bert layers
        for layer in BERT1.layers:
            for attr in ['kernel_regularizer']:
                if hasattr(layer, attr):
                    setattr(layer, attr, regularizer)
        for layer in BERT.layers:
            for attr in ['kernel_regularizer']:
                if hasattr(layer, attr):
                    setattr(layer, attr, regularizer)
        for layer in BERT2.layers:
            for attr in ['kernel_regularizer']:
                if hasattr(layer, attr):
                    setattr(layer, attr, regularizer)

        #create full model with pretrained bert and own layers on top
        model_bert_0 = build_bert(BERT, 7, max_len=MAXLEN)
        model_bert_1 = build_bert(BERT1, 22, max_len=MAXLEN)
        model_bert_2 = build_bert(BERT2, 3, max_len=MAXLEN)

    # train only the mlp layers of the bert model, leaving the pretrained layers unchanged
    history_bert = model_bert_0.fit(x=X_bert_train, y=y_train, class_weight=class_weight, batch_size=BATCH_SIZE,
                                    epochs=10, steps_per_epoch=steps_per_epoch, validation_data=[X_bert_test, y_test],
                                    verbose=1, )
    history_bert = model_bert_1.fit(x=X_bert_train, y=y_train, class_weight=class_weight, batch_size=BATCH_SIZE,
                                    epochs=10, steps_per_epoch=steps_per_epoch, validation_data=[X_bert_test, y_test],
                                    verbose=1, )
    history_bert = model_bert_2.fit(x=X_bert_train, y=y_train, class_weight=class_weight, batch_size=BATCH_SIZE,
                                    epochs=10, steps_per_epoch=steps_per_epoch, validation_data=[X_bert_test, y_test],
                                    verbose=1, )

    # train the mlp models 
    history_mlp = model_mlp_0.fit(x=X_mlp_train, y=y_train, epochs=100, validation_data=[X_mlp_test, y_test],
                                  class_weight=class_weight, callbacks=EarlyStoppingAtMinLoss_mlp())
    history_mlp = model_mlp_1.fit(x=X_mlp_train, y=y_train, epochs=100, validation_data=[X_mlp_test, y_test],
                                  class_weight=class_weight, callbacks=EarlyStoppingAtMinLoss_mlp())
    history_mlp = model_mlp_2.fit(x=X_mlp_train, y=y_train, epochs=100, validation_data=[X_mlp_test, y_test],
                                  class_weight=class_weight, callbacks=EarlyStoppingAtMinLoss_mlp())

    with strategy.scope():
        # Make BERT layers trainable
        for layer in BERT.layers:
            layer.trainable = True
        for layer in BERT1.layers:
            layer.trainable = True
        for layer in BERT2.layers:
            layer.trainable = True
            
        #recompile bert models now with the layers set to trainable and lower learning rate
        model_bert_0.compile(optimizer=tf.keras.optimizers.Adam(lr=5e-6),
                             loss=BinaryFocalLoss(gamma=2),
                             metrics=METRICS)
        model_bert_1.compile(optimizer=tf.keras.optimizers.Adam(lr=5e-6),
                             loss=BinaryFocalLoss(gamma=2),
                             metrics=METRICS)
        model_bert_2.compile(optimizer=tf.keras.optimizers.Adam(lr=5e-6),
                             loss=BinaryFocalLoss(gamma=2),
                             metrics=METRICS)

    #train the whole bert models
    history_bert2 = model_bert_0.fit(x=X_bert_train, y=y_train, class_weight=class_weight, batch_size=BATCH_SIZE,
                                     epochs=50, steps_per_epoch=steps_per_epoch, validation_data=[X_bert_test, y_test],
                                     verbose=1, callbacks=[EarlyStoppingAtMinLoss()])
    history_bert2 = model_bert_1.fit(x=X_bert_train, y=y_train, class_weight=class_weight, batch_size=BATCH_SIZE,
                                     epochs=50, steps_per_epoch=steps_per_epoch, validation_data=[X_bert_test, y_test],
                                     verbose=1, callbacks=[EarlyStoppingAtMinLoss()])
    history_bert2 = model_bert_2.fit(x=X_bert_train, y=y_train, class_weight=class_weight, batch_size=BATCH_SIZE,
                                     epochs=50, steps_per_epoch=steps_per_epoch, validation_data=[X_bert_test, y_test],
                                     verbose=1, callbacks=[EarlyStoppingAtMinLoss()])


    # predict with all 3 bert models and add the predictions to the arrays
    test_preds_bert = model_bert_0.predict(X_bert_test)
    test_preds_bert_array_0.append(test_preds_bert)
    test_preds_bert = model_bert_1.predict(X_bert_test)
    test_preds_bert_array_1.append(test_preds_bert)
    test_preds_bert = model_bert_2.predict(X_bert_test)
    test_preds_bert_array_2.append(test_preds_bert)

    # predict with all 3 mlp models and add the predictions to the arrays
    test_preds_mlp = model_mlp_0.predict(X_mlp_test)
    test_preds_mlp_array_0.append(test_preds_mlp)
    test_preds_mlp = model_mlp_1.predict(X_mlp_test)
    test_preds_mlp_array_1.append(test_preds_mlp)
    test_preds_mlp = model_mlp_2.predict(X_mlp_test)
    test_preds_mlp_array_2.append(test_preds_mlp)

    # create new model as ensemle model from the 3 mlp models
    models_mlp = [model_mlp_0, model_mlp_1, model_mlp_2]
    model_input_mlp = tf.keras.Input(shape=(6,))
    model_outputs_mlp = [model_mlp_0(model_input_mlp), model_mlp_1(model_input_mlp), model_mlp_2(model_input_mlp)]
    ensemble_output_mlp = HardMax()(model_outputs_mlp)
    ensemble_model_mlp = tf.keras.Model(inputs=model_input_mlp, outputs=ensemble_output_mlp)

     # create new model as ensemle model from the 3 bert models (see Xu et al., 2021)
    models_bert = [model_bert_0, model_bert_1, model_bert_2]
    model_inputs_bert = tf.keras.Input(shape=(MAXLEN,))
    model_outputs_bert = [model_bert_0(model_inputs_bert), model_bert_1(model_inputs_bert),
                          model_bert_2(model_inputs_bert)]
    ensemble_output_bert = HardMax()(model_outputs_bert)
    ensemble_model_bert = tf.keras.Model(inputs=model_inputs_bert, outputs=ensemble_output_bert)

    # test trained mlp ensemle model on validation dataset and seve results to array
    test_preds_ensemble = ensemble_model_mlp.predict(X_mlp_test)
    ensemble_mlp_preds_array.append(test_preds_ensemble)

    # test trained bert ensemle model on validation dataset and seve results to array
    test_preds_ensemble = ensemble_model_bert.predict(X_bert_test)
    ensemble_bert_preds_array.append(test_preds_ensemble)

    #create final model from the 2 ensemble models 
    models_final = [ensemble_model_bert, ensemble_model_mlp]
    model_input_final_bert = tf.keras.Input(shape=(MAXLEN,))
    model_input_final_mlp = tf.keras.Input(shape=(6,))
    model_outputs_final = [ensemble_model_bert(model_input_final_bert), ensemble_model_mlp(model_input_final_mlp)]
    ensemble_output_final = tf.keras.layers.Average()(model_outputs_final)
    ensemble_model_final = tf.keras.Model(inputs=[model_input_final_bert, model_input_final_mlp],
                                          outputs=ensemble_output_final)

    test_preds_ensemble = ensemble_model_final.predict([X_bert_test, X_mlp_test])
    ensemble_final_preds_array.append(test_preds_ensemble)

threshholds = list(range(0, 101, 1))
threshholds = [x / 100.0 for x in threshholds]


#custom plotting of ROC
def plot_roc(name, labels, predictions, **kwargs):
    fpr_array = []
    tpr_array = []
    for threshold in threshholds:
        current_preds = []
        for pred in predictions:
            if pred >= threshold:
                current_preds.append(1)
            else:
                current_preds.append(0)
        tp = 0
        fp = 0
        tn = 0
        fn = 0
        for l1, l2 in zip(current_preds, labels):
            if l1 == 1 and l2 == 0:
                fp += 1
            elif l1 == 1 and l2 == 1:
                tp += 1
            elif l1 == 0 and l2 == 0:
                tn += 1
            elif l1 == 0 and l2 == 1:
                fn += 1
        tpr = tp / (tp + fn)
        fpr = fp / (fp + tn)
        fpr_array.append(fpr)
        tpr_array.append(tpr)
    fpr_array = [x * 100.0 for x in fpr_array]
    tpr_array = [x * 100.0 for x in tpr_array]
    all_tp_arrays.append(tpr_array)
    all_fp_arrays.append(fpr_array)
    plt.plot(fpr_array, tpr_array, label=name, linewidth=2, **kwargs)
    plt.xlabel('False positives [%]')
    plt.ylabel('True positives [%]')
    plt.xlim([-0.5, 100.5])
    plt.ylim([-0.5, 100.5])
    plt.grid(True)
    ax = plt.gca()
    ax.set_aspect('equal')


#custom plotting of average ROC over 5 runs
def plot_roc_average(name, fpr_arrays, tpr_arrays, **kwargs):
    fpr_array = []
    tpr_array = []
    for x in range(len(fpr_arrays[0])):
        y = fpr_arrays[0][x] + fpr_arrays[1][x] + fpr_arrays[2][x] + fpr_arrays[3][x] + fpr_arrays[4][x]
        y = y / 5
        fpr_array.append(y)
        y = tpr_arrays[0][x] + tpr_arrays[1][x] + tpr_arrays[2][x] + tpr_arrays[3][x] + tpr_arrays[4][x]
        y = y / 5
        tpr_array.append(y)

    plt.plot(fpr_array, tpr_array, label=name, linewidth=2, **kwargs)
    plt.xlabel('False positives [%]')
    plt.ylabel('True positives [%]')
    plt.xlim([-0.5, 100.5])
    plt.ylim([-0.5, 100.5])
    plt.grid(True)
    ax = plt.gca()
    ax.set_aspect('equal')

#custom calculation of average AUC over 5 runs
def average_auc(labels, predictions):
    m = tf.keras.metrics.AUC()
    aucs_array = []
    average_auc = 0
    for x in range(len(labels)):
        m.update_state(labels[x], predictions[x])
        result = m.result().numpy()
        aucs_array.append(result)
    average_auc = round(sum(aucs_array)/len(aucs_array), 3)
    return average_auc
    

# plot the roc of the ensemble bert model over 5 runs as well as average and average auc
all_fp_arrays = []
all_tp_arrays = []
plot_roc("model1", test_labels[0], ensemble_bert_preds_array[0], color=colors[0], linestyle='--')
plot_roc("model2", test_labels[1], ensemble_bert_preds_array[1], color=colors[1], linestyle='--')
plot_roc("model3", test_labels[2], ensemble_bert_preds_array[2], color=colors[2], linestyle='--')
plot_roc("model4", test_labels[3], ensemble_bert_preds_array[3], color=colors[3], linestyle='--')
plot_roc("model5", test_labels[4], ensemble_bert_preds_array[4], color=colors[4], linestyle='--')
plot_roc_average("average", all_fp_arrays, all_tp_arrays, color="black", linestyle='-')
plt.legend(loc='lower right');
plt.text(62, 5,"Avg AUC:\n  {}".format(average_auc(test_labels,ensemble_bert_preds_array)), fontsize = 19, 
         bbox = dict(facecolor = 'white', alpha = 0.5))
plt.savefig('bert_ensemble_all.png')

plt.close()
plt.cla()
plt.clf()

# plot the roc of the ensemble mlp model over 5 runs as well as average and average auc
all_fp_arrays = []
all_tp_arrays = []
plot_roc("model1", test_labels[0], ensemble_mlp_preds_array[0], color=colors[0], linestyle='--')
plot_roc("model2", test_labels[1], ensemble_mlp_preds_array[1], color=colors[1], linestyle='--')
plot_roc("model3", test_labels[2], ensemble_mlp_preds_array[2], color=colors[2], linestyle='--')
plot_roc("model4", test_labels[3], ensemble_mlp_preds_array[3], color=colors[3], linestyle='--')
plot_roc("model5", test_labels[4], ensemble_mlp_preds_array[4], color=colors[4], linestyle='--')
plot_roc_average("average", all_fp_arrays, all_tp_arrays, color="black", linestyle='-')
plt.legend(loc='lower right');
plt.text(62, 5,"Avg AUC:\n  {}".format(average_auc(test_labels,ensemble_mlp_preds_array)), fontsize = 19, 
         bbox = dict(facecolor = 'white', alpha = 0.5))
plt.savefig('mlp_ensemle_all.png')

plt.close()
plt.cla()
plt.clf()

# plot the roc of the final ensemble model over 5 runs as well as average and average auc
all_fp_arrays = []
all_tp_arrays = []
plot_roc("model1", test_labels[0], ensemble_final_preds_array[0], color=colors[0], linestyle='--')
plot_roc("model2", test_labels[1], ensemble_final_preds_array[1], color=colors[1], linestyle='--')
plot_roc("model3", test_labels[2], ensemble_final_preds_array[2], color=colors[2], linestyle='--')
plot_roc("model4", test_labels[3], ensemble_final_preds_array[3], color=colors[3], linestyle='--')
plot_roc("model5", test_labels[4], ensemble_final_preds_array[4], color=colors[4], linestyle='--')
plot_roc_average("average", all_fp_arrays, all_tp_arrays, color="black", linestyle='-')
plt.legend(loc='lower right');
plt.text(62, 5,"Avg AUC:\n  {}".format(average_auc(test_labels,ensemble_final_preds_array)), fontsize = 19, 
         bbox = dict(facecolor = 'white', alpha = 0.5))
plt.savefig('final_ensemle_all.png')

plt.close()
plt.cla()
plt.clf()



